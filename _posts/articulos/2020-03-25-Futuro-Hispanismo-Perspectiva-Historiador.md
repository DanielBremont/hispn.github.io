---
layout: post
title: El futuro del hispanismo desde la perspectiva de un historiador
categories: [articulos]
---

El articulo argumenta a favor de un nuevo hispanismo historiografico a partir de la existencia de un  objeto de estudio propio diferente del definido por las historiografias nacionales.

<!--more-->
  
 <!-- Table of content -->
    <nav class="toc js-toc"></nav>

    <div class="js-toc-content">

        <h2 id="get-started">Get Started</h2>
        You can use npm to install it or include the script on the page with HTML.

        Oh created Hyukoh originally as a one-man band but in 2014, he formed the current four-piece band under the same name with bassist Im Dong-geon (임동건), guitarist Lim Hyun-jae (임현제) and drummer Lee In-woo (이인우). Oh worked with Lee in the past, and was introduced to Lee through Lim, with whom he had attended high school, and to Im through a mutual acquaintance.

        The band has released two EPs, 20 (2014) and 22 (2015), and one full-length studio album, 23 (2017). Oh is credited with the songwriting on all three albums, which are named after the band members' ages (all four band members were born in the year 1993). While the band achieved success in the Korean indie music scene with the release of their EPs, they gained mainstream popularity after participating in the South Korean television program Infinite Challenge. Following the release of 23, the band embarked on their first world tour. Currently the band is signed to DooRooDooRoo Artist Company(DRDR AC).

        <h3 id="include-js">Include JS</h3>
        Install it with npm.

        So you’re publishing a long document online and don’t have an easy mechanism to automatically add a
        table of contents on the server side? Well with JavaScript, you enslave the web browser to do it instead! This
        TOC generator…

        needs no jQuery or other third-party library
        creates links to each heading, adding an id attribute to each heading that doesn’t already have one
        can support pages that contain multiple content areas with multiple unrelated tables of contents

        <h2 id="examples">Examples</h2>
        The simplest thing you can do is to to call addTOC(document.body), but that only works on very simple pages.
        Generally the page has a main content area; if you assign an id="content" to this content area,
        you can add the table of contents at the top like this:



        <h3 id="examples3">Manuel Bremont</h3>
        The simplest thing you can do is to to call addTOC(document.body), but that only works on very simple pages.
        Generally the page has a main content area; if you assign an id="content" to this content area,
        you can add the table of contents at the top like this:

        In 1989 the College of Industrial Sciences, with eleven departments, was established at the second campus in Sejong. The preparation for the second campus was initiated by the purchase of a plot of land covering 151,250 square meters in Sejong. On April 23, 1988, the first phase of construction, including the lecture buildings, gymnasium, auditorium, and dormitories started. Also, sports facilities for baseball, soccer, and tennis were added. All these efforts culminated into 11 buildings as of March 1991.

        Further expansion took place from 1991 to 1994, with more departments and colleges being added to the University. The Department of Radio Science & Communication Engineering was established in the College of Engineering at the Seoul campus. The College of Industrial Sciences at the second campus was divided into the College of Science and Technology and the College of Visual Arts (now renamed the College of Design and Arts), where the latter included the Department of Industrial Crafts and the Department of Advertisement Design. The College of Business Management was also added during this period.

        In 1995 and 1996 Hongik University underwent a major restructuring of the departments of the colleges. Many departments exploring similar fields of study were merged into school systems, thereby allowing students to explore a wider spectrum. Additionally, three more graduate schools were established: the Graduate School of Industry at theSejong campus and the Graduate School of Advertising and Public Relations and the Graduate School of Educational Management at the Seoul campus. Also, in 1995, the university began to expand globally, becoming sister universities with over 32 institutions worldwide, including those in the United States, China, Russia, Japan, the United Kingdom, Spain, Finland, and Australia, among others.


        <h2 id="requirements">Requirements</h2>
        This library uses vanilla JavaScript.
        It is less than 350 bytes of CSS and about 3.6Kb of JavaScript (minified and gzipped) it also has no
        dependencies.
        This script works in all modern browsers and IE 9+.

        Hongik University, widely regarded as the best architecture and art school rivalry with Seoul National University in South Korea, 
        was founded by an independence activist in 1946. It is located in Mapo-gu district of central Seoul, 
        South Korea with a second campus in Sejong.[1] Hongik University has one of the most popular bachelor's 
        degree in architecture & design in South Korea. However the university also offers a comprehensive range of 
        undergraduate and graduate programs. As of 2007, the university was home to 14,500 undergraduate students and 2,600 graduate students, 
        and the undergraduate school consists of College of Fine Arts, College of Education, College of Engineering, 
        College of Liberal Arts, College of Architecture, College of Law, and College of Economics and Business Administration. 
        The graduate school provides research-based and practice-based programs in comprehensive fields including liberal arts, engineering, 
        fine arts and design, education, economics, performing arts, urban planning, architecture, film and photography. 
        The shortened term for Hongik University, "Hongdae," serves as a metonym for the neighborhood.

        <h2 id="nose1">Nose1</h2>
        In 2012 a Microsoft employee working on Bing had an idea about changing the way the search engine displayed ad headlines. Developing it wouldn’t require much effort—just a few days of an engineer’s time—but it was one of hundreds of ideas proposed, and the program managers deemed it a low priority. So it languished for more than six months, until an engineer, who saw that the cost of writing the code for it would be small, launched a simple online controlled experiment—an A/B test—to assess its impact. Within hours the new headline variation was producing abnormally high revenue, triggering a “too good to be true” alert. Usually, such alerts signal a bug, but not in this case. An analysis showed that the change had increased revenue by an astonishing 12%—which on an annual basis would come to more than $100 million in the United States alone—without hurting key user-experience metrics. It was the best revenue-generating idea in Bing’s history, but until the test its value was underappreciated.

Humbling! This example illustrates how difficult it can be to assess the potential of new ideas. Just as important, it demonstrates the benefit of having a capability for running many tests cheaply and concurrently—something more businesses are starting to recognize.

Today, Microsoft and several other leading companies—including Amazon, Booking.com, Facebook, and Google—each conduct more than 10,000 online controlled experiments annually, with many tests engaging millions of users. Start-ups and companies without digital roots, such as Walmart, Hertz, and Singapore Airlines, also run them regularly, though on a smaller scale. These organizations have discovered that an “experiment with everything” approach has surprisingly large payoffs. It has helped Bing, for instance, identify dozens of revenue-related changes to make each month—improvements that have collectively increased revenue per search by 10% to 25% each year. These enhancements, along with hundreds of other changes per month that increase user satisfaction, are the major reason that Bing is profitable and that its share of U.S. searches conducted on personal computers has risen to 23%, up from 8% in 2009, the year it was launched.

At a time when the web is vital to almost all businesses, rigorous online experiments should be standard operating procedure. If a company develops the software infrastructure and organizational skills to conduct them, it will be able to assess not only ideas for websites but also potential business models, strategies, products, services, and marketing campaigns—all relatively inexpensively. Controlled experiments can transform decision making into a scientific, evidence-driven process—rather than an intuitive reaction. Without them, many breakthroughs might never happen, and many bad ideas would be implemented, only to fail, wasting resources.

Yet we have found that too many organizations, including some major digital enterprises, are haphazard in their experimentation approach, don’t know how to run rigorous scientific tests, or conduct way too few of them.

Together we’ve spent more than 35 years studying and practicing experiments and advising companies in a wide range of industries about them. In these pages we’ll share the lessons we’ve gleaned about how to design and execute them, ensure their integrity, interpret their results, and address the challenges they’re likely to pose. Though we’ll focus on the simplest kind of controlled experiment, the A/B test, our findings and suggestions apply to more-complex experimental designs as well.

<h2 id="nose3">Appreciate the Value of A/B Tests</h2>

In an A/B test the experimenter sets up two experiences: “A,” the control, is usually the current system and considered the “champion,” and “B,” the treatment, is a modification that attempts to improve something—the “challenger.” Users are randomly assigned to the experiences, and key metrics are computed and compared. (Univariable A/B/C tests and A/B/C/D tests and multivariable tests, in contrast, assess more than one treatment or modifications of different variables at the same time.) Online, the modification could be a new feature, a change to the user interface (such as a new layout), a back-end change (such as an improvement to an algorithm that, say, recommends books at Amazon), or a different business model (such as an offer of free shipping). Whatever aspect of operations companies care most about—be it sales, repeat usage, click-through rates, or time users spend on a site—they can use online A/B tests to learn how to optimize it.

Any company that has at least a few thousand daily active users can conduct these tests. The ability to access large customer samples, to automatically collect huge amounts of data about user interactions on websites and apps, and to run concurrent experiments gives companies an unprecedented opportunity to evaluate many ideas quickly, with great precision, and at a negligible cost per incremental experiment. That allows organizations to iterate rapidly, fail fast, and pivot.

Recognizing these virtues, some leading tech companies have dedicated entire groups to building, managing, and improving an experimentation infrastructure that can be employed by many product teams. Such a capability can be an important competitive advantage—provided you know how to use it. Here’s what managers need to understand:

<h2 id="nose4">nose4</h2>

People commonly assume that the greater an investment they make, the larger an impact they’ll see. But things rarely work that way online, where success is more about getting many small changes right. Though the business world glorifies big, disruptive ideas, in reality most progress is achieved by implementing hundreds or thousands of minor improvements.

Putting credit card offers on the shopping cart page boosted profits by millions.

Consider the following example, again from Microsoft. (While most of the examples in this article come from Microsoft, where Ron heads experimentation, they illustrate lessons drawn from many companies.) In 2008 an employee in the United Kingdom made a seemingly minor suggestion: Have a new tab (or a new window in older browsers) automatically open whenever a user clicks on the Hotmail link on the MSN home page, instead of opening Hotmail in the same tab. A test was run with about 900,000 UK users, and the results were highly encouraging: The engagement of users who opened Hotmail increased by an impressive 8.9%, as measured by the number of clicks they made on the MSN home page. (Most changes to engagement have an effect smaller than 1%.) However, the idea was controversial because few sites at the time were opening links in new tabs, so the change was released only in the UK.

In June 2010 the experiment was replicated with 2.7 million users in the United States, producing similar results, so the change was rolled out worldwide. Then, to see what effect the idea might have elsewhere, Microsoft explored the possibility of having people who initiated a search on MSN open the results in a new tab. In an experiment with more than 12 million users in the United States, clicks per user increased by 5%. Opening links in new tabs is one of the best ways to increase user engagement that Microsoft has ever introduced, and all it required was changing a few lines of code. Today many websites, including Facebook.com and Twitter.com, use this technique.

Microsoft’s experience is hardly unique. Amazon’s experiments, for instance, revealed that moving credit card offers from its home page to the shopping cart page boosted profits by tens of millions of dollars annually. Clearly, small investments can yield big returns. Large investments, however, may have little or no payoff. Integrating Bing with social media—so that content from Facebook and Twitter opened on a third pane on the search results page—cost Microsoft more than $25 million to develop and produced negligible increases in engagement and revenue.

<h2 id="nose5">nose5</h2>

Online tests can help managers figure out how much investment in a potential improvement is optimal. This was a decision Microsoft faced when it was looking at reducing the time it took Bing to display search results. Of course, faster is better, but could the value of an improvement be quantified? Should there be three, 10, or perhaps 50 people working on that performance enhancement? To answer those questions, the company conducted a series of A/B tests in which artificial delays were added to study the effects of minute differences in loading speed. The data showed that every 100-millisecond difference in performance had a 0.6% impact on revenue. With Bing’s yearly revenue surpassing $3 billion, a 100-millisecond speedup is worth $18 million in annual incremental revenue—enough to fund a sizable team.

The test results also helped Bing make important trade-offs, specifically about features that might improve the relevance of search results but slow the software’s response time. Bing wanted to avoid a situation in which many small features cumulatively led to a significant degradation in performance. So the release of individual features that slowed the response by more than a few milliseconds was delayed until the team improved either their performance or the performance of another component.

<h2 id="nose6">nose6</h2>

More than a century ago, the department store owner John Wanamaker reportedly coined the marketing adage “Half the money I spend on advertising is wasted; the trouble is that I don’t know which half.” We’ve found something similar to be true of new ideas: The vast majority of them fail in experiments, and even experts often misjudge which ones will pay off. At Google and Bing, only about 10% to 20% of experiments generate positive results. At Microsoft as a whole, one-third prove effective, one-third have neutral results, and one-third have negative results. All this goes to show that companies need to kiss a lot of frogs (that is, perform a massive number of experiments) to find a prince.

Any figure that looks interesting or different is usually wrong.

It’s key to experiment with everything to make sure that changes neither are degrading nor have unexpected effects. At Bing about 80% of proposed changes are first run as controlled experiments. (Some low-risk bug fixes and machine-level changes like operating system upgrades are excluded.)

Scientifically testing nearly every proposed idea requires an infrastructure: instrumentation (to record such things as clicks, mouse hovers, and event times), data pipelines, and data scientists. Several third-party tools and services make it easy to try experiments, but if you want to scale things up, you must tightly integrate the capability into your processes. That will drive down the cost of each experiment and increase its reliability. On the other hand, a lack of infrastructure will keep the marginal costs of testing high and could make senior managers reluctant to call for more experimentation.

the growth of experimentation at Bing

Microsoft provides a good example of a substantial testing infrastructure—though a smaller enterprise or one whose business is not as dependent on the experimentation could make do with less, of course. Microsoft’s Analysis & Experimentation team consists of more than 80 people who on any given day help run hundreds of online controlled experiments on various products, including Bing, Cortana, Exchange, MSN, Office, Skype, Windows, and Xbox. Each experiment exposes hundreds of thousands—and sometimes even tens of millions—of users to a new feature or change. The team runs rigorous statistical analyses on all these tests, automatically generating scorecards that check hundreds to thousands of metrics and flag significant changes.

A company’s experimentation personnel can be organized in three ways:

Centralized model.
In this approach a team of data scientists serve the entire company. The advantage is that they can focus on long-term projects, such as building better experimentation tools and developing more-advanced statistical algorithms. One major drawback is that the business units using the group may have different priorities, which could lead to conflicts over the allocation of resources and costs. Another con is that data scientists may feel like outsiders when dealing with the businesses and thus be less attuned to the units’ goals and domain knowledge, which could make it harder for them to connect the dots and share relevant insights. Moreover, the data scientists may lack the clout to persuade senior management to invest in building the necessary tools or to get corporate and business unit managers to trust the experiments’ results.

Decentralized model.
Another approach is distributing data scientists throughout the different business units. The benefit of this model is that the data scientists can become experts in each business domain. The main disadvantage is the lack of a clear career path for these professionals, who also may not receive peer feedback and mentoring that help them develop. And experiments in individual units may not have the critical mass to justify building the required tools.

<h2 id="nose7">nose7</h2>
A third option is to have some data scientists in a centralized function and others within the different business units. (Microsoft uses this approach.) A center of excellence focuses mostly on the design, execution, and analysis of controlled experiments. It significantly lowers the time and resources those tasks require by building a companywide experimentation platform and related tools. It can also spread best testing practices throughout the organization by hosting classes, labs, and conferences. The main disadvantages are a lack of clarity about what the center of excellence owns and what the product teams own, who should pay for hiring more data scientists when various units increase their experiments, and who is responsible for investments in alerts and checks that indicate results aren’t trustworthy.

There is no right or wrong model. Small companies typically start with the centralized model or use a third-party tool and then, after they’ve grown, switch to one of the other models. In companies with multiple businesses, managers who consider testing a priority may not want to wait until corporate leaders develop a coordinated organizational approach; in those cases, a decentralized model might make sense, at least in the beginning. And if online experimentation is a corporate priority, a company may want to build expertise and develop standards in a central unit before rolling them out in the business units.
        

    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js"></script>

    <script type="text/javascript">
        tocbot.init({
            // Where to render the table of contents.
            tocSelector: '.js-toc',
            // Where to grab the headings to build the table of contents.
            contentSelector: '.js-toc-content',
            // Which headings to grab inside of the contentSelector element.
            headingSelector: 'h1, h2, h3, h4',
            // For headings inside relative or absolute positioned containers within content.
            hasInnerContainers: true,
        });
    </script>

    <div style="height: 495;">Final</div>